[package]
name = "kv-cache-tier"
version = "0.1.0"
edition = "2021"
authors = ["databloom"]
description = "Tiered KV-cache for LLM inference: GPU VRAM → Host RAM → Disk, enabling near-infinite context length on consumer GPUs"
license = "MIT"
repository = "https://github.com/databloom/ollama-kv-cache-tiering"
keywords = ["llm", "kv-cache", "gpu", "inference", "ollama"]
categories = ["science", "hardware-support"]

[dependencies]
# HTTP server
axum = "0.8"
tokio = { version = "1", features = ["full"] }
tower = "0.5"
tower-http = { version = "0.6", features = ["cors", "trace"] }

# Async
futures = "0.3"
async-trait = "0.1"

# GPU (optional — build without for CPU-only testing)
cudarc = { version = "0.16", optional = true }

# Compression
zstd = "0.13"

# Serialization
serde = { version = "1", features = ["derive"] }
serde_json = "1"

# Logging
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "json"] }

# FP16 types
half = { version = "2", features = ["serde"] }

# CLI
clap = { version = "4", features = ["derive"] }

# Metrics
prometheus = "0.14"

# Error handling
thiserror = "2"
anyhow = "1"

# UUID for request IDs
uuid = { version = "1", features = ["v4"] }

# Byte utilities
bytes = "1"
bytemuck = { version = "1", features = ["derive"] }

# Time
tokio-stream = "0.1"

[dev-dependencies]
tempfile = "3"
criterion = { version = "0.5", features = ["html_reports"] }
reqwest = { version = "0.12", features = ["json"] }

[features]
default = []
cuda = ["dep:cudarc"]

[profile.release]
opt-level = 3
lto = "thin"
codegen-units = 1

[[bench]]
name = "cache_bench"
harness = false
